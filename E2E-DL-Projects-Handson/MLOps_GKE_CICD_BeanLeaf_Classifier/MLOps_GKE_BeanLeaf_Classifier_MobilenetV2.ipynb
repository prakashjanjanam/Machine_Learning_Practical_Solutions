{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLOps_GKE_Bean_Leaf_Image_Classification_Transfer_Learning_MobileNet",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCI-CurG0BTG"
      },
      "outputs": [],
      "source": [
        "# This is transfer learning with Mobilenet V2\n",
        "# https://www.youtube.com/watch?v=84J1fMklQWE\n",
        "# Downloads Feature Extractor from Tensorflow Hib (tfhub.dev)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yg9dGo6QP5xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Emyx-ZdpPFZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_HOME = \"/content/gdrive/MyDrive\""
      ],
      "metadata": {
        "id": "VN08EvDJRyyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Collab Notebook is by default at /content location?? Let's find out later\n",
        "# Switch to Dataset path and get data into it\n",
        "#!pwd\n",
        "#!ls\n",
        "#%cd $DRIVE_HOME/Technical/Coding/Kaggle_datasets/Bean_Classification\n",
        "!pwd\n"
      ],
      "metadata": {
        "id": "Kac0t9glRjsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Technical/Coding/Kaggle_datasets'"
      ],
      "metadata": {
        "id": "MQGeqWys0eVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Kaggle API Documenttation for configuring Access Credentials - https://github.com/Kaggle/kaggle-api\n",
        "# You can find/generate API access token by going to Profile->Account section\n",
        "# Once generated, it downloads the Kaggle JSON. Note the details there and run following\n",
        "api_token_json_str = {\"username\":\"prakashjanjanam\",\"key\":\"f97185b3563b44ca62b499d473e3c4b0\"}\n",
        "#!mkdir ~/.kaggle\n",
        "#!touch ~/.kaggle/kaggle.json\n",
        "with open('/root/.kaggle/kaggle.json','w') as f:\n",
        "  json.dump(api_token_json_str, f)\n",
        " "
      ],
      "metadata": {
        "id": "PuNHFjDfPDSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ~/.kaggle/kaggle.json /content/gdrive/MyDrive/Technical/Coding/Kaggle_datasets\n",
        "!kaggle datasets download -d prakharrastogi534/bean-leaf-dataset"
      ],
      "metadata": {
        "id": "uAFfYHNoc_Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the Dataset to extract train/test/validation folders\n",
        "!pwd\n",
        "!ls -lart\n",
        "!unzip bean-leaf-dataset.zip"
      ],
      "metadata": {
        "id": "XguDHNdy0v6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!find . -type f | wc -l # Total Images\n",
        "!find ./train -type f | wc -l # Train Images\n",
        "!find ./test -type f | wc -l # Test Set\n",
        "!find ./validation -type f | wc -l # Validation Set"
      ],
      "metadata": {
        "id": "MaNEGM4UhvCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See Sample Images\n",
        "display(Image(\"/content/drive/MyDrive/Technical/Coding/Kaggle_datasets/Bean_Classification/train/train/angular_leaf_spot/angular_leaf_spot_train.0.jpg\"))"
      ],
      "metadata": {
        "id": "WQSmg6Aei5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will do Transfer learning (with custom fine tuning with final few layers)\n",
        "# In ths exercise, we will use pre-trained mobile_net V2 model\n",
        "batch_size = 128\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "metadata": {
        "id": "qe6ZfwrjjJaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's prepare the Datasets using Keras Preprocessing\n",
        "# API Which will resize the images and prepare datasets as batches\n",
        "# This will make it compatible to be fed into the model as a step itself\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory('train/train',\n",
        "  seed=111,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "BwsewW0sj-Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory('test/test',\n",
        "  seed=111,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "11w1Jugaj-QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory('validation/validation',\n",
        "  seed=111,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "vbo2eT9Ok0EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory('train/train',\n",
        "  seed=111,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "EgUZd8aCj-Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, label_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(label_batch.shape)\n"
      ],
      "metadata": {
        "id": "_HhsJEwpj-hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what claases are available\n",
        "classes = train_ds.class_names\n",
        "print(classes)\n",
        "# Let's print 1st 10 images of 1st Batch\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(classes[labels[i]])\n",
        "    plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "q_gBqTjtlanE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUTO TUNE for effective Batch extraction and caching through Prefetch\n",
        "# That saves time for Training\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "OhuaIlk2lap2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF Hub is a Model Zoo with all Pre-trained MOdels(Text, Image Problem domains)\n",
        "# Feature Vector is a trained instance of network that only gets feaqture vectors and not final classification layers\n",
        "# These final layers, we will add and train\n",
        "feature_extractor = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\""
      ],
      "metadata": {
        "id": "yXNna0qAl9m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_layer = hub.KerasLayer(feature_extractor, input_shape=(img_height,img_width,3))"
      ],
      "metadata": {
        "id": "YsYf8vDsl9q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't want to retrain the MOdel for now, hence re using as-is\n",
        "# Unless your data is completely different, we cna reuse and add final dense/classificationn layers to only train those last layers\n",
        "# If you think data is completely different, we can use architecture and re-train all layers\n",
        "feature_extractor_layer.trainable = False"
      ],
      "metadata": {
        "id": "jyOhU_dAlasK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_layer.trainable = False"
      ],
      "metadata": {
        "id": "3QZCY7CBlavC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization Layer works faster to converge with normalized data\n",
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
      ],
      "metadata": {
        "id": "eaeS20KpoChr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matches my Dataset preparation steps, the seed given above there\n",
        "tf.random.set_seed(111)"
      ],
      "metadata": {
        "id": "QKHd9wbvoCnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are using Pre-trained Mode Vector above\n",
        "# Last Dropout added for reducing Overfitting\n",
        "# Last layer adds 3 output predictions as we needed\n",
        "model = tf.keras.Sequential([\n",
        "  normalization_layer,\n",
        "  feature_extractor_layer,\n",
        "  tf.keras.layers.Dropout(0.3),\n",
        "  tf.keras.layers.Dense(3,activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "w4QLHpV8oCwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse Categorical cross entropy to be used when we didn't one hot encode the target\n",
        "# Where as if we did ONe-hot encoding, we use just CategoricalCross Entropy\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "AHCsSiZQlawN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This can be trained for 10 or 20 Epochs. Training is pretty fast as it's only last layer training\n",
        "# You could do an Early stopping call back if needed and accuracy doesn't improve\n",
        "history = model.fit(train_ds, epochs=20, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "7MN-lbEXlay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that Rescaling (Pre-processing) is In-built to the modle itself\n",
        "# So, if we deploy the graph object, entire pre-processing is also deployed\n",
        "# There are lot of Pre-processing options within Keras within computation graph of the model\n",
        "# You can do one hot encoding, Standardization within the graph itself\n",
        "# THis is new feature after 2.4\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "39ovpQb4qQAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_acc', 'val_acc'], loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aPs6OOZrqQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now go and evaluate with test dataset\n",
        "# It's close to Train, Validation Accuracy. So, it's decent enough\n",
        "result = model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "hqVCapTXqQIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize MOdel output with Actual and Predicted Labels\n",
        "# 1 Batch, 9 Images in Subplots\n",
        "# Since we are ineferring individual image here, we need to pre-process it\n",
        "# As Array and add the Batch Dim that the model expects\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in test_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    img = tf.keras.preprocessing.image.img_to_array(images[i])                    \n",
        "    img = np.expand_dims(img, axis=0)  \n",
        "\n",
        "    pred=model.predict(img)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(\"Actual Label: %s\" % classes[labels[i]])\n",
        "    plt.text(1, 240, \"Predicted Label: %s\" % classes[np.argmax(pred)], fontsize=12)\n",
        "\n",
        "    plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "RnLbrFu3qQME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving in .TF format to dpeloy to CI-CD pipeline\n",
        "# TF is the latets format. We can save in old h5 format too\n",
        "model.save('./models', save_format='tf')\n"
      ],
      "metadata": {
        "id": "1LLkSVH3qQO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that from the saved directory, this is what the contents infer\n",
        "# Saved_model.pb is the Graph Object; Keras_metadata.pb is overall Keras meta data including Pre-processing function\n",
        "# TODO - Load the folder structure\n",
        "!pwd\n",
        "!ls -lart models"
      ],
      "metadata": {
        "id": "nEoX2AKAqQR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load abd verify the model now\n",
        "model_loaded = tf.keras.models.load_model('./models/')\n",
        "model_loaded.summary()"
      ],
      "metadata": {
        "id": "_HD5plDCqQVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are using PIL, SkImage for Image Pre-processing\n",
        "# Before passing to INference\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage import transform\n",
        "def process(filename):\n",
        "   np_image = Image.open(filename)\n",
        "   np_image = np.array(np_image).astype('float32')\n",
        "   np_image = transform.resize(np_image, (224, 224, 3))\n",
        "   np_image = np.expand_dims(np_image, axis=0)\n",
        "   return np_image\n"
      ],
      "metadata": {
        "id": "P4tYymaStVqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label=model_loaded.predict(process('train/train/healthy/healthy_train.0.jpg'))\n",
        "print(classes[np.argmax(pred_label)])\n"
      ],
      "metadata": {
        "id": "Jvy0A6j1tVtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label"
      ],
      "metadata": {
        "id": "NpuMElT2tVwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use this Zipped model to deploy to CI-CD\n",
        "!zip -r models.zip models/"
      ],
      "metadata": {
        "id": "8y3EMyLvuFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to ensure that the TF version used to build this model\n",
        "# Matches what we will use in container while serving\n",
        "# TF APIs are aggressively changing, we just wnat to make sure\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "zKqC-DSAuF23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lart"
      ],
      "metadata": {
        "id": "2TnUOVGZwIgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SCm-dC0fwJXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}